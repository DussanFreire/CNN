{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica de CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grupo:\n",
    "* Dussan Freire\n",
    "* Jhonny Camacho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links Utilizados\n",
    "* __https://keras.io/examples/vision/mnist_convnet/__\n",
    "* __https://victorzhou.com/blog/keras-cnn-tutorial/__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import mnist\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow numpy mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up para los experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "tar: Error opening archive: Failed to open 'MNIST.tar.gz'\n",
      "C:\\Users\\Dussan\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:64: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "C:\\Users\\Dussan\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:54: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "C:\\Users\\Dussan\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:69: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\Dussan\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:59: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz\n",
    "\n",
    "train_set = MNIST('./', download=True,\n",
    "transform=transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "]), train=True)\n",
    "\n",
    "# Obtener sets\n",
    "test_set = MNIST('./', download=True,\n",
    "transform=transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "]), train=False)\n",
    "\n",
    "train_images = train_set.train_data\n",
    "train_labels = train_set.train_labels\n",
    "test_images = train_set.test_data\n",
    "test_labels = train_set.test_labels\n",
    "\n",
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 1\n",
    "Se aumento la cantidad de filtros de 8 a 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteristicas:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros utilizados\n",
    "num_filters = 15\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.3085 - accuracy: 0.9122 - val_loss: 0.1604 - val_accuracy: 0.9552\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1371 - accuracy: 0.9614 - val_loss: 0.1027 - val_accuracy: 0.9712\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0968 - accuracy: 0.9722 - val_loss: 0.0745 - val_accuracy: 0.9785\n",
      "tiempo_de_entrenamiento:  69.440239 [s]\n"
     ]
    }
   ],
   "source": [
    "# Contruccion del modelo\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compilacion del modelo\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Crear una closure para poder medir el tiempo\n",
    "def run_experiment1():\n",
    "      model.fit(\n",
    "      train_images,\n",
    "      to_categorical(train_labels),\n",
    "      epochs=epochs,\n",
    "      validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "# Entrenar el modela\n",
    "execution_time = timeit.timeit(run_experiment1, number=1)\n",
    "\n",
    "print(\"tiempo_de_entrenamiento: \", execution_time,\"[s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('experimeto1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "[5, 0, 4, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5].numpy().tolist()) \n",
    "# print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluision\n",
    "Se mejoro el resultado y tomo un poco mas de tiempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento numero 2\n",
    "Se aumento tamano del filro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteriscidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros utilizados\n",
    "num_filters = 8\n",
    "filter_size = 10\n",
    "pool_size = 2\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3416 - accuracy: 0.9014 - val_loss: 0.1935 - val_accuracy: 0.9441\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1513 - accuracy: 0.9557 - val_loss: 0.1142 - val_accuracy: 0.9666\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.1046 - accuracy: 0.9686 - val_loss: 0.0809 - val_accuracy: 0.9768\n",
      "tiempo_de_entrenamiento:  81.579613 [s]\n"
     ]
    }
   ],
   "source": [
    "# Contruccion del modelo\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compilacion del modelo\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Crear una closure para poder medir el tiempo\n",
    "def run_experiment1():\n",
    "      model.fit(\n",
    "      train_images,\n",
    "      to_categorical(train_labels),\n",
    "      epochs=epochs,\n",
    "      validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "# Entrenar el modela\n",
    "execution_time = timeit.timeit(run_experiment1, number=1)\n",
    "\n",
    "print(\"tiempo_de_entrenamiento: \", execution_time,\"[s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('experimeto2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "[5, 0, 4, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5].numpy().tolist()) \n",
    "# print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluision\n",
    "La exatitud de la ultima capa 0.9686 en un tiempo de 91.57 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento numero 3\n",
    "Se agrando el pool size de 2 a 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteriscidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros utilizados\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 4\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.4443 - accuracy: 0.8730 - val_loss: 0.1886 - val_accuracy: 0.9450\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.1586 - accuracy: 0.9535 - val_loss: 0.1289 - val_accuracy: 0.9628\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.1217 - accuracy: 0.9641 - val_loss: 0.1088 - val_accuracy: 0.9678\n",
      "tiempo_de_entrenamiento:  69.82293539999998 [s]\n"
     ]
    }
   ],
   "source": [
    "# Contruccion del modelo\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compilacion del modelo\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Crear una closure para poder medir el tiempo\n",
    "def run_experiment1():\n",
    "      model.fit(\n",
    "      train_images,\n",
    "      to_categorical(train_labels),\n",
    "      epochs=epochs,\n",
    "      validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "# Entrenar el modela\n",
    "execution_time = timeit.timeit(run_experiment1, number=1)\n",
    "\n",
    "print(\"tiempo_de_entrenamiento: \", execution_time,\"[s]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('experimeto3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "[5, 0, 4, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5].numpy().tolist()) \n",
    "# print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluision\n",
    "La exatitud de la ultima capa 0.9641 en un tiempo de 69.82 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento numero 4\n",
    "Se aumentaron las epocas de 3 a 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteriscidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros utilizados\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.3519 - accuracy: 0.8974 - val_loss: 0.2116 - val_accuracy: 0.9384\n",
      "Epoch 2/6\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.1742 - accuracy: 0.9497 - val_loss: 0.1470 - val_accuracy: 0.9573\n",
      "Epoch 3/6\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.1284 - accuracy: 0.9630 - val_loss: 0.1057 - val_accuracy: 0.9695\n",
      "Epoch 4/6\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.1032 - accuracy: 0.9705 - val_loss: 0.0889 - val_accuracy: 0.9747\n",
      "Epoch 5/6\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 0.0877 - accuracy: 0.9744 - val_loss: 0.0732 - val_accuracy: 0.9791\n",
      "Epoch 6/6\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0767 - accuracy: 0.9777 - val_loss: 0.0615 - val_accuracy: 0.9826\n",
      "tiempo_de_entrenamiento:  181.0324925 [s]\n"
     ]
    }
   ],
   "source": [
    "# Contruccion del modelo\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compilacion del modelo\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Crear una closure para poder medir el tiempo\n",
    "def run_experiment1():\n",
    "      model.fit(\n",
    "      train_images,\n",
    "      to_categorical(train_labels),\n",
    "      epochs=epochs,\n",
    "      validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "# Entrenar el modela\n",
    "execution_time = timeit.timeit(run_experiment1, number=1)\n",
    "\n",
    "print(\"tiempo_de_entrenamiento: \", execution_time,\"[s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('experimeto4.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "[5, 0, 4, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5].numpy().tolist()) \n",
    "# print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluision\n",
    "La exatitud de la ultima capa 0.9777 en un tiempo de 181.03 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento  5\n",
    "Se aumento una capa de conv2d de 1 a 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteriscidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros utilizados\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1875/1875 [==============================] - 53s 28ms/step - loss: 0.2864 - accuracy: 0.9151 - val_loss: 0.1455 - val_accuracy: 0.9578\n",
      "Epoch 2/6\n",
      "1875/1875 [==============================] - 58s 31ms/step - loss: 0.1215 - accuracy: 0.9655 - val_loss: 0.0936 - val_accuracy: 0.9718\n",
      "Epoch 3/6\n",
      "1875/1875 [==============================] - 57s 31ms/step - loss: 0.0896 - accuracy: 0.9733 - val_loss: 0.0790 - val_accuracy: 0.9755\n",
      "Epoch 4/6\n",
      "1875/1875 [==============================] - 78s 42ms/step - loss: 0.0744 - accuracy: 0.9768 - val_loss: 0.0603 - val_accuracy: 0.9812\n",
      "Epoch 5/6\n",
      "1875/1875 [==============================] - 67s 36ms/step - loss: 0.0666 - accuracy: 0.9795 - val_loss: 0.0552 - val_accuracy: 0.9833\n",
      "Epoch 6/6\n",
      "1875/1875 [==============================] - 80s 43ms/step - loss: 0.0589 - accuracy: 0.9818 - val_loss: 0.0501 - val_accuracy: 0.9840\n",
      "tiempo_de_entrenamiento:  394.74025040000004 [s]\n"
     ]
    }
   ],
   "source": [
    "# Contruccion del modelo\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compilacion del modelo\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Crear una closure para poder medir el tiempo\n",
    "def run_experiment1():\n",
    "      model.fit(\n",
    "      train_images,\n",
    "      to_categorical(train_labels),\n",
    "      epochs=epochs,\n",
    "      validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "# Entrenar el modela\n",
    "execution_time = timeit.timeit(run_experiment1, number=1)\n",
    "\n",
    "print(\"tiempo_de_entrenamiento: \", execution_time,\"[s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('experimeto5.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002064A9ABDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[5 0 4 1 9]\n",
      "[5, 0, 4, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5].numpy().tolist()) \n",
    "# print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluision\n",
    "La exatitud de la ultima capa 0.9818 en un tiempo de 394.74. segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento 6\n",
    "Se aumento una capa \"dropout\", para evitar overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteriscidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros utilizados\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1875/1875 [==============================] - 33s 17ms/step - loss: 0.4670 - accuracy: 0.8581 - val_loss: 0.2589 - val_accuracy: 0.9261\n",
      "Epoch 2/6\n",
      "1875/1875 [==============================] - 25s 14ms/step - loss: 0.2938 - accuracy: 0.9125 - val_loss: 0.1958 - val_accuracy: 0.9442\n",
      "Epoch 3/6\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.2527 - accuracy: 0.9246 - val_loss: 0.1639 - val_accuracy: 0.9532\n",
      "Epoch 4/6\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.2306 - accuracy: 0.9307 - val_loss: 0.1484 - val_accuracy: 0.9571\n",
      "Epoch 5/6\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.2151 - accuracy: 0.9353 - val_loss: 0.1290 - val_accuracy: 0.9637\n",
      "Epoch 6/6\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2015 - accuracy: 0.9388 - val_loss: 0.1178 - val_accuracy: 0.9664\n",
      "tiempo_de_entrenamiento:  168.1937379000001 [s]\n"
     ]
    }
   ],
   "source": [
    "# Contruccion del modelo\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compilacion del modelo\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Crear una closure para poder medir el tiempo\n",
    "def run_experiment1():\n",
    "      model.fit(\n",
    "      train_images,\n",
    "      to_categorical(train_labels),\n",
    "      epochs=epochs,\n",
    "      validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "# Entrenar el modela\n",
    "execution_time = timeit.timeit(run_experiment1, number=1)\n",
    "\n",
    "print(\"tiempo_de_entrenamiento: \", execution_time,\"[s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('experimeto6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002064A9ABB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[5 0 4 1 9]\n",
      "[5, 0, 4, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5].numpy().tolist()) \n",
    "# print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluision\n",
    "La exatitud de la ultima capa 0.9388 en un tiempo de 168.193 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento  7\n",
    "Se aumento una capa anterior a la capa de salida de 64 unidades de activacion utilizando la funcion relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteriscidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros utilizados\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.2469 - accuracy: 0.9282 - val_loss: 0.1073 - val_accuracy: 0.9689\n",
      "Epoch 2/6\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0982 - accuracy: 0.9706 - val_loss: 0.0658 - val_accuracy: 0.9804\n",
      "Epoch 3/6\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0664 - accuracy: 0.9802 - val_loss: 0.0443 - val_accuracy: 0.9869\n",
      "Epoch 4/6\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0494 - accuracy: 0.9848 - val_loss: 0.0305 - val_accuracy: 0.9911\n",
      "Epoch 5/6\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.0389 - accuracy: 0.9875 - val_loss: 0.0246 - val_accuracy: 0.9925\n",
      "Epoch 6/6\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0306 - accuracy: 0.9902 - val_loss: 0.0176 - val_accuracy: 0.9949\n",
      "tiempo_de_entrenamiento:  158.47807669999997 [s]\n"
     ]
    }
   ],
   "source": [
    "# Contruccion del modelo\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compilacion del modelo\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Crear una closure para poder medir el tiempo\n",
    "def run_experiment1():\n",
    "      model.fit(\n",
    "      train_images,\n",
    "      to_categorical(train_labels),\n",
    "      epochs=epochs,\n",
    "      validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "# Entrenar el modela\n",
    "execution_time = timeit.timeit(run_experiment1, number=1)\n",
    "\n",
    "print(\"tiempo_de_entrenamiento: \", execution_time,\"[s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('experimeto7.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002064BDB5430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[5 0 4 1 9]\n",
      "[5, 0, 4, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5].numpy().tolist()) \n",
    "# print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluision\n",
    "La exatitud de la ultima capa 0.9902 en un tiempo de 158.478 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento  8\n",
    "Se jugo con los parametros de la capa de convulucion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteriscidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros utilizados\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "epochs = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.4924 - accuracy: 0.8670 - val_loss: 0.2338 - val_accuracy: 0.9308\n",
      "Epoch 2/6\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2042 - accuracy: 0.9398 - val_loss: 0.1794 - val_accuracy: 0.9463\n",
      "Epoch 3/6\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.1594 - accuracy: 0.9524 - val_loss: 0.1394 - val_accuracy: 0.9588\n",
      "Epoch 4/6\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.1352 - accuracy: 0.9597 - val_loss: 0.1197 - val_accuracy: 0.9647\n",
      "Epoch 5/6\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1206 - accuracy: 0.9640 - val_loss: 0.1108 - val_accuracy: 0.9671\n",
      "Epoch 6/6\n",
      "1875/1875 [==============================] - 10s 6ms/step - loss: 0.1102 - accuracy: 0.9670 - val_loss: 0.1120 - val_accuracy: 0.9649\n",
      "tiempo_de_entrenamiento:  70.64834719999999 [s]\n"
     ]
    }
   ],
   "source": [
    "# Contruccion del modelo\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,\n",
    "    padding='same',\n",
    "    activation='relu',\n",
    "  ),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compilacion del modelo\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Crear una closure para poder medir el tiempo\n",
    "def run_experiment1():\n",
    "      model.fit(\n",
    "      train_images,\n",
    "      to_categorical(train_labels),\n",
    "      epochs=epochs,\n",
    "      validation_data=(test_images, to_categorical(test_labels)))\n",
    "\n",
    "# Entrenar el modela\n",
    "execution_time = timeit.timeit(run_experiment1, number=1)\n",
    "\n",
    "print(\"tiempo_de_entrenamiento: \", execution_time,\"[s]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('experimeto8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002064A3998B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[5 0 4 1 9]\n",
      "[5, 0, 4, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) \n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5].numpy().tolist()) \n",
    "# print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concluision\n",
    "La exatitud de la ultima capa 0.9670 en un tiempo de 170.64 segundos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento sacado del link:\n",
    "__https://keras.io/examples/vision/mnist_convnet/__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros del modelo \n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "batch_size = 128\n",
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desarrollo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "60000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train = train_set.train_data\n",
    "y_train = train_set.train_labels\n",
    "x_test  = train_set.test_data\n",
    "y_test = train_set.test_labels\n",
    "\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# Convertir a matrices binarias\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "# Contruccion del modelo\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                16010     \n",
      "=================================================================\n",
      "Total params: 34,826\n",
      "Trainable params: 34,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "422/422 [==============================] - 42s 99ms/step - loss: 0.3628 - accuracy: 0.8897 - val_loss: 0.0816 - val_accuracy: 0.9773\n",
      "Epoch 2/15\n",
      "422/422 [==============================] - 41s 98ms/step - loss: 0.1138 - accuracy: 0.9650 - val_loss: 0.0594 - val_accuracy: 0.9832\n",
      "Epoch 3/15\n",
      "422/422 [==============================] - 37s 89ms/step - loss: 0.0881 - accuracy: 0.9729 - val_loss: 0.0484 - val_accuracy: 0.9860\n",
      "Epoch 4/15\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 0.0728 - accuracy: 0.9773 - val_loss: 0.0437 - val_accuracy: 0.9882\n",
      "Epoch 5/15\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 0.0659 - accuracy: 0.9792 - val_loss: 0.0417 - val_accuracy: 0.9888\n",
      "Epoch 6/15\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 0.0581 - accuracy: 0.9818 - val_loss: 0.0374 - val_accuracy: 0.9900\n",
      "Epoch 7/15\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 0.0531 - accuracy: 0.9825 - val_loss: 0.0367 - val_accuracy: 0.9897\n",
      "Epoch 8/15\n",
      "422/422 [==============================] - 41s 97ms/step - loss: 0.0489 - accuracy: 0.9845 - val_loss: 0.0369 - val_accuracy: 0.9895\n",
      "Epoch 9/15\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 0.0472 - accuracy: 0.9852 - val_loss: 0.0345 - val_accuracy: 0.9907\n",
      "Epoch 10/15\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 0.0438 - accuracy: 0.9862 - val_loss: 0.0303 - val_accuracy: 0.9908\n",
      "Epoch 11/15\n",
      "422/422 [==============================] - 39s 91ms/step - loss: 0.0412 - accuracy: 0.9865 - val_loss: 0.0316 - val_accuracy: 0.9905\n",
      "Epoch 12/15\n",
      "422/422 [==============================] - 39s 93ms/step - loss: 0.0390 - accuracy: 0.9872 - val_loss: 0.0299 - val_accuracy: 0.9920\n",
      "Epoch 13/15\n",
      "422/422 [==============================] - 38s 90ms/step - loss: 0.0378 - accuracy: 0.9883 - val_loss: 0.0306 - val_accuracy: 0.9925\n",
      "Epoch 14/15\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 0.0363 - accuracy: 0.9885 - val_loss: 0.0285 - val_accuracy: 0.9935\n",
      "Epoch 15/15\n",
      "422/422 [==============================] - 37s 88ms/step - loss: 0.0352 - accuracy: 0.9884 - val_loss: 0.0292 - val_accuracy: 0.9917\n",
      "Test loss: 0.01687825657427311\n",
      "Test accuracy: 0.9950500130653381\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "# Evaluacion del modelo entrenado\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.01687825657427311\n",
      "Test accuracy: 0.9950500130653381\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
